{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawData_file_path = './long_info370_assignment1_rawData.csv'\n",
    "cleanData_file_path = './long_info370_assignment1_cleanData.csv'\n",
    "calculated_file_path = './long_info370_assignment1_calculate.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "# Scrapes the website of interest, gather the required text data, \n",
    "# and place it into the correct format. This script outputs results to rawData_file_path\n",
    "\n",
    "# Pages to be scaped\n",
    "# Note: It takes really a long time to run webdriver on each page and wait for the ads loading.\n",
    "#       Can comment out the rest and leave only one page for testing purpose\n",
    "pages = [\n",
    "#         'https://www.yummly.com/recipes/lunch',\n",
    "#         'https://www.yummly.com/recipes/dinner',\n",
    "#         'https://www.yummly.com/browse/recommended',\n",
    "#         'https://www.yummly.com/browse/seasonal',\n",
    "#         'https://www.yummly.com/browse/popular-now',\n",
    "        'https://www.yummly.com/browse/quick-and-easy'\n",
    "        ]\n",
    "\n",
    "# Download webdriver from: https://chromedriver.storage.googleapis.com/index.html?path=2.33/\n",
    "# and put it in the same directory of the scripts\n",
    "# Download selenium from: https://pypi.python.org/pypi/selenium\n",
    "driver_path = os.path.dirname(os.path.abspath(\"__file__\")) + '/chromedriver'\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "f = open(rawData_file_path, 'w')\n",
    "writer = csv.writer(f, delimiter=',')\n",
    "\n",
    "# recipe page base url\n",
    "base_url = 'https://www.yummly.com/#recipe/'\n",
    "\n",
    "for page in pages:\n",
    "\n",
    "    # Request webpage information\n",
    "    page_req = requests.get(page)\n",
    "    page_soup = BeautifulSoup(page_req.text, 'html.parser')\n",
    "    recipe_grids = page_soup.find_all(\"div\", {\"class\" : \"single-recipe\"})\n",
    "\n",
    "    first_time=True\n",
    "    for recipe_grid in recipe_grids: # go to each recipe from the links in recipe_grids\n",
    "        recipe_link = base_url + recipe_grid.get('data-url')\n",
    "        page = driver.get(recipe_link)\n",
    "\n",
    "        # Sleep for page loading: first time 3 secs, and 1 secs for the rest\n",
    "        if first_time:\n",
    "            time.sleep(3)\n",
    "            first_time = False\n",
    "        else:\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Extract the information    \n",
    "        recipe_page = driver.page_source\n",
    "        recipe_soup = BeautifulSoup(recipe_page, 'html.parser')\n",
    "        if recipe_soup.find(\"div\", {\"class\" : \"primary-info-text\"}):\n",
    "            recipe_name = recipe_soup.find(\"div\", {\"class\" : \"primary-info-text\"}).find({\"h1\"}).get_text()\n",
    "            ingredient_elements = recipe_soup.find_all(\"span\", {\"class\" : \"ingredient\"})\n",
    "            ingredients = [(*map(lambda ele:ele.get_text(), ingredient_elements))]\n",
    "            for ingredient in ingredients:         \n",
    "                writer.writerow([recipe_link, recipe_name, ingredient])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Takes the data in rawData_file_path and clean it. This includes \n",
    "# removing excess white spaces, correcting for all edge cases, and correcting any remaining formatting issues. \n",
    "# This script should output results to cleanData_file_path\n",
    "f = open(cleanData_file_path, 'w')\n",
    "writer = csv.writer(f, delimiter=',')\n",
    "\n",
    "with open(rawData_file_path, \"r\") as csvDataFile:\n",
    "    cleanedRow = ''  \n",
    "    row_set = []  # Check for duplicate entries\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    for row in csvReader:\n",
    "        if row not in row_set:\n",
    "            cleanedRow = [(*map(lambda ele:ele.strip().lower(), row))] # all to lower case and strip any whitespace \n",
    "            writer.writerow(cleanedRow)\n",
    "            row_set.append(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import csv\n",
    "\n",
    "\n",
    "# Takes data in cleanData_file_path and calculate the top 10 \n",
    "# most frequently occurring ingredients in the ingredient list and output to calculated_file_path\n",
    "with open(cleanData_file_path, \"r\") as csvDataFile:\n",
    "    csvReader = csv.reader(csvDataFile)\n",
    "    word_freq_dict = {} # for calculating overall counts\n",
    "    word_prop_dict = {} # for calculating proportion\n",
    "    recipes_num = 0\n",
    "    recipe_set =[]\n",
    "    for row in csvReader:\n",
    "        recipe = row[1]\n",
    "        # Calculate total number of recipes\n",
    "        if recipe not in recipe_set:\n",
    "            recipe_set.append(recipe) \n",
    "            recipes_num += 1\n",
    "\n",
    "        words = row[2].split()\n",
    "        words_list = [] # for looking for word duplicates in one row\n",
    "        for word in words:\n",
    "            # Update occurrences of the word in word_freq_dict\n",
    "            if word not in word_freq_dict:\n",
    "                word_freq_dict[word] = 1\n",
    "            else:\n",
    "                word_freq_dict[word] += 1\n",
    "\n",
    "            # only count word once if there're duplicates in one row\n",
    "            if word not in words_list:\n",
    "                if word not in word_prop_dict:\n",
    "                    word_prop_dict[word] = 1\n",
    "                else:\n",
    "                    word_prop_dict[word] += 1\n",
    "                words_list.append(word)\n",
    "\n",
    "# Sort word_prop_dict based on word occurrences    \n",
    "sorted_word_prop_dict = sorted(word_prop_dict.items(), key=operator.itemgetter(1))\n",
    "desc_sorted_word_prop_dict = tuple(reversed(sorted_word_prop_dict))\n",
    "\n",
    "f = open(calculated_file_path, 'w')\n",
    "writer = csv.writer(f, delimiter=',')\n",
    "\n",
    "for x in range(0,10): # top 10\n",
    "    word = desc_sorted_word_prop_dict[x][0]\n",
    "    prop = int((word_prop_dict[word])) / (recipes_num)\n",
    "    count = word_freq_dict[word]\n",
    "    writer.writerow([word, count, prop])\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
